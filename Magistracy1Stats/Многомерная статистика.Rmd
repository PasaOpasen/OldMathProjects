---
title: "Лабораторная по многомерной статистике"
author: "Дмитрий Пасько"
date: "07.10.2019"
output: 
   html_document:
     toc: yes
     toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,include = TRUE,tidy = TRUE,cache = FALSE,eval = TRUE, message = FALSE,warning = FALSE,fig.align = "center")
```

```{r,echo=FALSE}
library(knitr)
setwd("C:\\Users\\крендель\\Desktop\\MagicCode\\Magistracy1Stats")
```


## Описание

Для выполнения работы использовался язык **R** версии `r getRversion()`. Необходимые пакеты скачиваются командой
```{r,eval=FALSE}
install.packages(c("readxl","dplyr","ggplot2","ggpubr","corrplot",
                   "psych","MASS","tree","randomForest"))
```
и подключаются
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(corrplot)
library(psych)
library(MASS)
library(tree)
library(randomForest)
```

Также использовался номер варианта
```{r}
#номер варианта
nv=7
```
Дополнительная информация: [огромная статья по построению моделей в R](https://ranalytics.github.io/data-mining/01-Data-Mining-Models-in-R.html), [cтатья по кластерному, факторному и дискриминантному анализу в R](https://rpubs.com/iezepov/e502lec7), [курс по статистике в R](https://stepik.org/course/524/syllabus), [перечень основых функций языка](https://aakinshin.net/ru/posts/r-functions/), [математические операции в R](https://ru.wikibooks.org/wiki/Язык_программирования_R/Математика), [книги по R на моём GitHub](https://github.com/PasaOpasen/LittleHelps/tree/master/книги%20по%20языкам/R).

***
## Многомерный анализ

### Задание 1
Подготовим данные:

```{r}
datacrude =data.frame(read_excel("Таблица 1.xlsx")) #считывание таблицы
data=datacrude[5:nrow(datacrude),-1]#удаление лишних строк и столбцов
data=data[-nv,]#удаление строки в соответствиии с номером варианта
colnames(data)=c("Country","Doctors","Deaths","GDP","Costs")#переименование столбцов

data[,2:5]=apply(data[,2:5],2,function(x)scale(as.numeric(x)))#тут переменные из текста преобразуются в числа и стандартизируются
data[,1]=factor(data[,1])#первая переменная из количественной преобразуется в номенативную
```
Полученная таблица:
```{r}
data_frame(data)
```

Для решения задачи создается матрица (евклидовых) расстояний
```{r}
d = dist(data[,2:5], method = "euclidean")#матрица расстояний
```
которая используется функцией кластеризации по методу ближайшего соседа с расстоянием Варда между кластерами:
```{r}
fit <- hclust(d, method = "ward.D")
```
Дендрограмма полученной кластеризации:
```{r,fig.height=7}
plot(fit, labels = data$Country,xlab = "Countries")
```

и сумма внутригрупповых расстояний по мере объединения кластеров:
```{r}
plot(fit$height, xlab ="step",ylab="dist",type="b",col="blue",lwd=1,main="Расстояния при объединении кластеров")
```

### Задание 2
Попробуем узнать, сколько кластеров будет достаточно. Для этого рассчитаем суммы внутригрупповых расстояний, когда число кластеров равно 1, 2, ... 8, и изобразим их на графике:
```{r}
it=1:8
sums=sapply(it, function(k) kmeans(data[,2:5], k)$tot.withinss)
plot(it,sums,type = "b",col="red",main = "Суммы внутригрупповых расстояний при разном числе кластеров")
```

По принципу метода каменистой осыпи делаем вывод, что исходный набор данных естественно делится на 2 или 3 кластера. Напишем функцию, которая строит модель для заданного числа кластеров, проводит анализ этой модели и строит некоторые графики:
```{r chunk1}
#функция, проводящая некоторый анализ и строящая графики для заданного числа кластеров
getimage=function(k){
  fit=kmeans(data[,2:5],k)#строится модель
cat("Внутригрупповые суммы:",fit$withinss,"\n")#внутригрупповые суммы
cat("Общая сумма:", fit$betweenss,"\n") 
cat("Матрица расстояний:\n")
print(dist(fit$centers))#матрица расстояний

#Добавляем кластер к фрейму данных
library(dplyr)
newdata=as_data_frame(data)%>%mutate(cluster=factor(fit$cluster))

#агрегирование данных по группам
means=newdata[,2:6]%>%group_by(cluster)%>%summarise(
  meanCosts=mean(Costs),sdCosts=sd(Costs),
  meanDoctors=mean(Doctors),sdDoctors=sd(Doctors),
  meanGDP=mean(GDP),sdGDP=sd(GDP),
  meanDeaths=mean(Deaths),sdDeaths=sd(Deaths)
  )
print(means)


means=means[,c(1,2,4,6,8)]#берётся сабсет только из значений для средних

lbs=c("cluster1","cluster2","cluster3","cluster4","cluster5")

library(ggplot2)
library(ggpubr)

#здесь создаётся таблица со средними по каждой переменной и каждому классу в том виде, в каком удобней рисовать
tmpdata=data.frame(x=1:4,means=as.numeric(means[1,2:5]),cluster=rep(lbs[1],4))
for(i in 2:k){
  tmpdata=rbind(tmpdata,data.frame(x=1:4,means=as.numeric(means[i,2:5]),cluster=rep(lbs[i],4)))
}
tmpdata$cluster=factor(tmpdata$cluster)

ppp=ggplot(tmpdata,aes(x=x,y=means,col=cluster))+
  geom_line()+
  geom_point(size=4)


pl1=ggplot(newdata, aes(x=Doctors, y=Deaths, col = cluster))+
  geom_point(size = 3)+
  theme_bw() 

pl2=ggplot(newdata, aes(x=GDP, y=Costs, col = cluster))+
  geom_point(size = 3)+
  theme_bw()

pl3=ggplot(newdata, aes(x=GDP, y=Deaths, col = cluster))+
  geom_point(size = 3)+
  theme_bw()
pl4=ggplot(newdata, aes(x=GDP, y=Doctors, col = cluster))+
  geom_point(size = 3)+
  theme_bw()

costs = ggplot(newdata, aes(x=cluster, y=Costs))+
  geom_boxplot()+
  theme_bw()
deaths = ggplot(newdata, aes(x=cluster, y=Deaths))+
  geom_boxplot()+
  theme_bw()
doctors = ggplot(newdata, aes(x=cluster, y=Doctors))+
  geom_boxplot()+
  theme_bw()
gdp = ggplot(newdata, aes(x=cluster, y=GDP))+
  geom_boxplot()+
  theme_bw()

p1 <- ggarrange(pl1, pl2,pl3,pl4,
                ncol = 2, nrow = 2)
p2 <- ggarrange(costs, deaths, doctors, gdp,
                ncol = 2, nrow = 2)
ggarrange(ppp,p1, p2, ncol = 1, nrow = 3,heights=c(1.3,2,3))
}
```

Используем эту функцию для двух кластеров:
```{r,cache=TRUE,fig.height=10}
getimage(2)
```

Видно, что исходные наблюдения хорошо разделяются на две группы. Если использовать три кластера, такое разделение будет уже сомнительным:
```{r,cache=TRUE,fig.height=10}
getimage(3)
```

Отсюда вывод: достаточно было использовать только два кластера.


### Задание 3

Загрузим данные:
```{r}
datacrude =data.frame(read_excel("Приложение 1.xlsx")) 
data=datacrude[,-c(1)]
data=data[,-c(1,2,16,17)]
data_frame(data)
```

Визуализация корреляционной матрицы заданного набора переменных:
```{r}
library(corrplot)
corrplot(cor(data))
```

В наборе данных в целом нет значительных корреляций, поэтому сжать его до трёх-четырёх главных компонент без сильной потери информации не получится.

Результаты аналаза методом главных компонент без вращения:
```{r}
library(psych)
principal(data[,-1],nfactors = 8,rotate = "none") #Создание модели
```

Здесь сперва выведена матрица корреляций, затем **SS loadings** --- это собственные значения каждой компоненты, **Proportion Var** --- доля дисперсий, объясняемых каждой компонентой, **Cumulative Var** --- кумулятивная доля (первая компонента объясняет 20%, первые две вместе --- 34%, первые три --- 46% и т. д.), дальше то же самое, только в пропорции. По принципу Кайзера следует выделить только 5 компонент, хотя, судя по объяснённым дисперсиям, и восьми может быть недостаточно.

Построим диаграмму каменистой осыпи:
```{r,message=FALSE}
fa.parallel(data[,-1],fa="pc",show.legend = T,main="Диаграмма каменистой осыпи с параллельным анализом")

```

Здесь, как и ожидалось, не наблюдается резкого убывания собственных значений, поэтому нельзя сказать, сколько конкретно главных компонент следовало бы выделить. Возмём 6 факторов и проведём анализ с вращением осей:
```{r}
#варимакс с нормализацией
(vm=principal(apply(data[,-1],2,scale),nfactors = 6,rotate = "varimax"))
```

По результатам анализа видно, что первая компонента сильно коррелирует с переменной Х13 (среднегодовой фонд заработной платы), пятая --- с Х6 (удельный вес покупных изделий) и Х15 (оборачиваемость нормированных оборотных средств), третья --- с Х10 (фондоотдача) и Х9 (удельный вес потерь от брака), вторая --- с Х11 (среднегодовая численность ППП), четвёртая --- с Х5 (удельный вес рабочих в составе ППП), шестая --- с Х7 (коэффициент сменности оборудования).

Также можно увидеть матрицу весовых коэффициентов:
```{r}
round(unclass(vm$weights),2)
```
а корреляционная матрица для главных компонент показывает их ортогональность:
```{r}
cor(vm$scores)
```

### Задание 4
Загрузим данные:
```{r}
data =data.frame(read_excel("Приложение 2.xlsx")) 
data$CLASS=factor(data$CLASS)
head(data,9)
```
Проведём обучение через линейный дискриминантный анализ:
```{r, include=FALSE}
library(MASS)
ldadat <- lda(CLASS~.,data,method="t")
```
```{r, eval=FALSE}
library(MASS)
ldadat <- lda(CLASS~.,data,method="t")
```
Характеристики модели:
```{r}
ldadat$means#групповые средние
(mat=ldadat$scaling)#матрица дискриминантных функций
```
Напишем функцию для удобной оценки ошибки обучения
```{r}
#функция для оценки ошибки 
misclass <- function(pred, obs) {
   tbl <- table(pred, obs)
   sum <- colSums(tbl)
   dia <- diag(tbl)
   msc <- (sum - dia)/sum * 100
   m.m <- mean(msc)
   cat("Classification table:", "\n")
   print(tbl)
   cat("Misclassification errors:", "\n")
   print(round(msc, 1))
  }
```
И применим её для значений, предсказанных моделью на обучающей выборке:
```{r}
misclass(predict(ldadat, data[,1:7])$class, data[,8])
```
Видно, что для некоторых переменных имеется ошибка более чем в 30%, что означает негодность выбранной модели обучения. Модель "Breiman's random forest" справляется с этой задачей лучше:
```{r,fig.height=7,fig.width=7}
library(randomForest)
rf <- randomForest(data[,8] ~ ., data=data[,1:7])
rfp <- predict(rf, data[,1:7])
MDSplot(randomForest(data[,-8]), data[,8])
misclass(rfp, data[,8])
```
Ошибка нулевая, все данные отнесены правильно. Правило, по которому наблюдение относится в ту или иную группу примерно сделующее:
```{r}
library(tree)
datatree <- tree(data[,8] ~ ., data[,-8])
plot(datatree)
text(datatree) 
```

### Задание 5

Считаем тестовые данные и проведём их классификацию по уже построенной модели:
```{r}
data2 =data.frame(read_excel("Приложение 3.xlsx")) 
data2= apply(data2,2,as.numeric)
data2=data2[31:80,]
cluster=predict(rf, data2)
data2=data.frame(cbind(data2,cluster))
data2$cluster=factor(data2$cluster)

head(data2,10)
```
Построим диаграммы ядерной плотности для распределения каждой переменной X1-X7 по отнесенным группам, чтобы убедиться в правильности классификации:
```{r,fig.height=9}
library(ggplot2)
library(ggpubr)

ggarrange(
  ggplot(data2,aes(x=X1,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X2,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X3,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X4,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X5,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X6,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X7,fill=cluster))+
    geom_density(alpha=0.6),
          ncol = 2, nrow = 4)
```

Другой вариант --- ящики с усами:
```{r,fig.height=9}
ggarrange(
  ggplot(data2,aes(x=cluster,y=X1))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X2))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X3))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X4))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X5))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X6))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X7))+
    geom_boxplot(),
  ncol = 2, nrow = 4)
```
















***

## Временные ряды