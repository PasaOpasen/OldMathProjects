\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[a4paper, top=2cm, bottom=2cm, left=1cm, right=1cm, marginparwidth=1.75cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage[english, russian]{babel}
\usepackage[section,above,below]{placeins}
\usepackage[noend]{algorithmic}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}
\section{Методы кластерного анализа}
\section{Иерархические алгоритмы}
\section{Расстояния между кластерами}

{\it Возможные расстояния}:
\begin{enumerate}
    \item \textbf{Расстояние "ближайшего соседа"} (одиночная связь): $$\rho_{\min}(K_i,K_j)=\min_{x_i \in K_i, x_j \in K_j} \rho (x_i,x_j).$$ Равно расстоянию между самыми ближними объектами кластеров.
    \item \textbf{Расстояние "дальнего соседа"} (полная связь):$$\rho_{\max}(K_i,K_j)=\max_{x_i \in K_i, x_j \in K_j} \rho (x_i,x_j).$$ Равно расстоянию между самыми дальними объектами кластеров.
    \item \textbf{Невзвешенное попарное среднее}: $$\rho_{\text{mean}}(K_i,K_j)=\text{mean}_{x_i \in K_i, x_j \in K_j} \rho (x_i,x_j).$$ Равно среднему между всеми парами расстояний.
    \item \textbf{Взвешенное попарное среднее}: $$\rho_{\text{mean2}}(K_i,K_j)=\text{mean}_{x_i \in K_i, x_j \in K_j} \rho (k_1 \cdot x_i,k_2 \cdot x_j) = \text{mean}_{x_i \in K_i, x_j \in K_j} \dfrac{k_1 x_i + k_2 x_j}{k_1+k_2}.$$ Равно среднему между всеми парами расстояний с учетом весов $k_1,k_2$, равных ёмкости кластеров.
    \item \textbf{Незвешенный центроидный метод}: расстояние между кластерами равно расстоянию между их центрами тяжести, где центр тяжести есть среднее арифметическое всех объектов в кластере: $$x_c = \dfrac{\sum_{i=1}^k 1 \cdot x_i}{k}= \text{mean}_{x_i \in K_i} (x_i)$$
    \item \textbf{Взвешенный центроидный метод}. Как я понял, это расстояние между центрами, но с учетом весов, как в пункте 4. Но вообще начиная с пункта 4 нигде нет формул, так что не факт, что они у меня правильные.
    \item \textbf{Метод Варда}. Целевой функцией является внутригрупповая сумма квадратов: $$\text{SS}=\sum_i \sum_{x_j \in K_i} \rho(x_j,x_i),$$
    где сумма берётся по всем кластерам, $x_i$ -- среднее по кластеру $K_i$. Объединение в кластеры на каждой итерации происходит так, чтобы увеличение этой функции было минимальным.
\end{enumerate}

\section{Процедуры эталонного типа}
Наряду с иерархическими методами кластеризации существуют итеративные ($k$-средних), суть которых заключается в следующем (возможны модификации):
\begin{enumerate}
    \item Среди объектов $x_i$ некоторым образом выбираются $k$ штук -- центры будущих кластеров.
    \item Объекты, не отнесённые к какому-либо кластеру, приписываются тому кластеру, до которого будет наименьшее расстрояние.
    \item Центры кластеров пересчитываются.
    \item Для всех точек пересматривается их принадлежность к кластеру.
    \item Пункты 3-4 повторяются, пока точки могут менять принадлежность.
\end{enumerate}
На этом сайте хорошо показано, как работает метод $k$-средних: \url{https://www.naftaliharris.com/blog/visualizing-k-means-clustering/}

\section{Методика дискриминантного анализа}
\section{Что характеризует Лямбда Уилкса?}
\section{Что показывают квадраты расстояний Махаланобиса?}
\section{Какое максимальное число канонических дискриминантных функций допустимо в дискриминантном анализе?}
\section{Какую информацию дают стандартизованные и структурные коэффициенты дискриминантной функции?}
 \textbf{Стандартизованные коэффициенты}  -- это то же самое, что и нестандартизованные, но полученные по стандартизованным начальным данным. \textit{Стандартизованные коэффициенты полезно применять для уменьшения размерности исходного признакового пространства переменных: если абсолютная величина коэффициента для данной переменной для всех дискриминантных функций мала, то эту переменную можно исключить, тем самым сократив число переменных.} 

\textbf{Структурные коэффициенты}  определяются коэффициентами взаимной корреляции между отдельными переменными и дискриминантной функцией. Если относительно некоторой переменной абсолютная величина коэффициента велика, то вся информация о дискриминантной функции заключена в этой переменной. 

Структурные коэффициенты по своей информативности несколько отличаются от стандартизованных коэффициен-тов. \textit{Стандартизованные коэффициенты показывают вклад переменных в значение дискриминантной функции}. Если две переменные сильно коррелированы, то их стандартизованные коэффициенты могут быть меньше по сравнению с теми случаями, когда используется только одна из этих переменных. Такое распределение величины стандартизованного коэффициента объясняется тем, что при их вычислении учитывается влияние всех переменных. \textit{Структурные же коэффициенты являются парными корреляциями и на них не влияют взаимные зависимости прочих переменных}.

\section{Опишите процедуру отбора переменных с помощью стандартизованных и структурных коэффициентов}
Никакой инфы про эту процедуру "отбора переменных с помощью стандартизованных и структурных коэффициентов" я не нашёл. Структурные коэффициенты вообще показывают корреляцию между переменной и дискриминантной функцией, что позволяет {\it интепретировать} результаты, а не отбирать переменные. Можете сами попробовать поискать что-то про это в копии её методички: \url{http://masters.donntu.org/2005/kita/kapustina/library/discr_an.htm}.

{\it Про стандартизованные коэффициенты есть такая инфа}: "В рамках алгоритмического подхода список информативных
переменных формируется автоматически в соответствии с тем
или иным алгоритмом пошагового дискриминантного анализа.
Предусмотрены два его варианта: «Forward stepwise» и
«Backward stepwise» –- метод последовательного пополнения и
метод последовательного исключения списка информативных
признаков. В первом случае выбирается переменная, вносящая
наибольший вклад в межгрупповые различия, и далее к ней на
каждом шаге анализа присоединятся другие информативные переменные. Метод исключения основан на альтернативной процедуре: из полного списка признаков на каждом шаге анализа последовательно исключаются малозначимые переменные." При этом величина вклада переменной в различия определяется как раз величиной стандартизованного коэффициента.

\textbf{Пошаговый дискриминантный анализ}.

Вероятно, наиболее общим применением дискриминантного анализа является включение в исследование многих переменных с целью определения тех из них, которые наилучшим образом разделяют совокупности между собой.

\textbf{Модель}. Другими словами, вы хотите построить "модель", позволяющую лучше всего предсказать, к какой совокупности будет принадлежать тот или иной образец. В следующем рассуждении термин "в модели" будет использоваться для того, чтобы обозначать переменные, используемые в предсказании принадлежности к совокупности; о неиспользуемых для этого переменных будем говорить, что они "вне модели".

\textbf{Пошаговый анализ с включением}. В пошаговом анализе дискриминантных функций модель дискриминации строится по шагам. Точнее, на каждом шаге просматриваются все переменные и находится та из них, которая вносит наибольший вклад в различие между совокупностями. Эта переменная должна быть включена в модель на данном шаге, и происходит переход к следующему шагу.

\textbf{Пошаговый анализ с исключением}. Можно также двигаться в обратном направлении, в этом случае все переменные будут сначала включены в модель, а затем на каждом шаге будут устраняться переменные, вносящие малый вклад в предсказания. Тогда в качестве результата успешного анализа можно сохранить только "важные" переменные в модели, то есть те переменные, чей вклад в дискриминацию больше остальных.

\section{Какова интерпретация канонического коэффициента корреляции?}
Общее число дискриминантных функций не превышает числа дискриминантных переменных и, по крайней мере, на единицу меньше числа групп. Степень разделения выборочных групп зависит от величины собственных чисел: чем больше собственное число, тем сильнее разделение. Наибольшей разделительной способностью обладает первая дискриминантная функция, соответствующая наибольшему собственному числу $\lambda_i$, вторая обеспечивает максимальное различение после первой и т. д. Различительную способность $i$-й функции оценивают по относительной величине в процентах собственного числа  $\lambda_i$ от суммы всех  $\lambda$.

\textbf{Коэффициент канонической корреляции}. Другой характеристикой, позволяющей оценить полезность дискриминантной функции является коэффициент канонической корреляции  $r_i$. Каноническая корреляция является мерой связи между двумя множествами переменных. Максимальная величина этого коэффициента равна 1. Будем считать, что группы составляют одно множество, а другое множество образуют дискриминантные переменные. Коэффициент канонической корреляции для $i$-й дискриминантной функции определяется формулой: $$r_i = \sqrt{\dfrac{\lambda_i}{1+\lambda_i}}.$$

\textbf{Чем больше величина  $r_i$, тем лучше разделительная способность дискриминантной функции.}

\section{В каком случае учет априорных вероятностей может сильно изменить результаты классификации?}
Априорные вероятности оказывают наибольшее влияние при перекрытии групп и, следовательно, многие объекты с большой вероятностью могут принадлежать ко многим группам. Если группы сильно различаются, то учет априорных вероятностей практически не влияет на результат классификации, поскольку между классами будет находиться очень мало объектов
\section{Методика факторного анализа}

\textbf{Факторный анализ} -- раздел статистического многомерного анализа, объединяющий методы оценки размерности множества наблюдаемых переменных посредством исследования структуры ковариационных или корреляционных матриц.

Факторный анализ (ФА) представляет собой совокупность методов, которые на основе реально существующих связей анализируемых признаков, связей самих наблюдаемых объектов, позволяют выявлять скрытые (неявные, латентные) обобщающие характеристики организационной структуры и механизма развития изучаемых явлений, процессов. Основное предположение факторного анализа заключается в том, что корреляционные связи между большим количеством наблюдаемых переменных можно объяснить существованием меньшего числа гипотетических переменных или факторов, называемых общими скрытыми факторами. Далее их будем называть просто факторами.
Факторный анализ может быть:
\begin{itemize}
    \item разведочным — он осуществляется при исследовании скрытой факторной структуры без предположения о числе факторов и их нагрузках;
    \item конфирматорным, предназначенным для проверки гипотез о числе факторов и их нагрузках.
\end{itemize}

\textbf{Фактор} – латентная переменная, конструируемая таким образом, чтобы можно было объяснить корреляцию между набором некоторых имеющихся переменных.
\textbf{Нагрузка} – корреляция между исходной переменной и фактором.

Общей моделью факторного анализа служит следующая линейная зависимость:
\begin{equation}
    x_i = \sum_{j=1}^m a_{i j} F_j+ b_i U_i + \varepsilon_i, i = 1, \dots, k,
\end{equation}
где $F_j$ -- общие факторы, $x_i$ -- наблюдаемые переменные, $U_i$ -- характерные факторы, $ \varepsilon_i$ -- случайные ошибки.

\begin{figure}[h!]
    \center{\includegraphics[width=0.9\linewidth]{factors.png}}
    \caption{Схема}
    \label{ris:image}
    \end{figure}

Главными целями факторного анализа являются:
\begin{itemize}
    \item сокращение числа переменных (редукция данных);
    \item определение структуры взаимосвязей между переменными, т.е. классификация переменных.
\end{itemize}
­	
­Практическое выполнение факторного анализа начинается с проверки его условий. В обязательные условия факторного анализа входят:
\begin{itemize}
    \item все признаки должны быть количественными;
    \item число наблюдений должно быть не менее чем в два раза больше числа переменных;
    \item выборка должна быть однородна.
\end{itemize}

Для проведения факторного анализа предлагается методика, включающая в себя следующие этапы:
\begin{enumerate}
    \item сбор исходных статистических данных и подготовка корреляционной (ковариационной) матрицы;
    \item выделение общих скрытых факторов;
    \item вращение факторной структуры;
    \item содержательная интерпретация результатов факторного анализа.
\end{enumerate}

Алгоритмы факторного анализа основываются на использовании редуцированной матрицы парных корреляций (ковариаций).
\textbf{Редуцированная матрица} – это матрица парных коэффициентов корреляции, на главной диагонали которой расположены не единицы (оценки) полной корреляции или оценки полной дисперсии, а их редуцированные, несколько уменьшенные величины – значения оценок общностей. Общность характеризует вклад данного признака в суммарную общность процесса
При этом постулируется, что в результате анализа будет объяснена не вся дисперсия изучаемых признаков (объектов), а ее некоторая часть, обычно большая. Оставшаяся необъясненная часть дисперсии — это характерность, возникающая из-за специфичности наблюдаемых объектов, или ошибок, допускаемых при регистрации явлений, процессов, т.е. ненадежности вводных данных.
	


\section{Суть задачи вращения общих факторов}
Задача вращения общих факторов решается с целью улучшения их {\it интерпретируемости}. Производится попытка достижения простой структуры, в которой каждая переменная характеризуется преобладающим влиянием какого–то одного фактора. Факторные нагрузки могут быть изображены в виде диаграммы рассеяния, на которой каждая переменная представлена точкой. Можно повернуть оси в любом направлении без изменения относительного положения точек. При этом действительные координаты точек, то есть факторные нагрузки, изменяются. 
\begin{figure}[h!]
    \center{\includegraphics[width=0.5\linewidth]{rotate.png}}
    \caption{Вращение факторов}
    \label{ris:image}
    \end{figure}

Существуют различные методы вращения факторов. Целью этих методов является получение понятной (интерпретируемой) матрицы нагрузок, то есть факторов, которые ясно отмечены высокими нагрузками для некоторых переменных и низкими -- для других. 

{\it Примеры методов вращения}:
\begin{itemize}
    \item­	\textbf{Варимакс}. Ортогональный метод вращения, минимизирующий число переменных с высокими нагрузками на каждый фактор. 
    \item ­	\textbf{Метод косоугольного (неортогонального) вращения}. Самое косоугольное решение соответствует дельте, равной 0 (по умолчанию). По мере того, как дельта отклоняется в отрицательную сторону, факторы становятся более ортогональными.
    \item ­	\textbf{Квартимакс}. Метод вращения, который минимизирует число факторов, необходимых для объяснения каждой переменной.
    \item ­	\textbf{Эквимакс}. Метод вращения, объединяющий методы варимакс, упрощающий факторы, и квартимакс, упрощающий переменные. Минимизируется число переменных с большими факторными нагрузками и число факторов, требуемых для объяснения переменной.
    \item ­	\textbf{Промакс-вращение}. Косоугольное вращение в предположении, что факторы могут коррелировать между собой. Оно производится быстрее, чем вращение типа косоугольного вращения, поэтому оно полезно для больших наборов данных.
\end{itemize}

\textbf{Примечание}. КОСОУГОЛЬНОЕ ВРАЩЕНИЕ --- такая трансформация факторного пространства, которая предполагает возможность проведения факторных осей под углами друг к другу, отличающимися от 90 градусов (от ортогональности)

\section{Критерий Кайзера}
\textbf{Критерий Кайзера или критерий собственных чисел}: отбираются только факторы с собственными значениями равными или большими 1. Это означает, что если фактор не выделяет дисперсию, эквивалентную, по крайней мере, дисперсии одной переменной, то он опускается. Этот критерий предложен Кайзером (Kaiser, 1960), и является, вероятно, наиболее широко используемым.
\section{Критерий Каменистой осыпи}
\section{Метод главных компонент}

\end{document}