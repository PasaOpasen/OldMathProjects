---
title: "Лабораторная по многомерной статистике"
author: "Дмитрий Пасько"
date: "07.10.2019"
output: 
   html_document:
     toc: yes
     toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,include = TRUE,tidy = TRUE,cache = FALSE,eval = TRUE, message = FALSE,warning = FALSE,fig.align = "center")
```

```{r,echo=FALSE}
library(knitr)
setwd("C:\\Users\\крендель\\Desktop\\MagicCode\\Magistracy1Stats")
```


## Описание

Для выполнения работы использовался язык **R** версии `r getRversion()`. Необходимые пакеты скачиваются командой
```{r,eval=FALSE}
install.packages(c("readxl","dplyr","ggplot2","ggpubr","corrplot",
                  "psych","MASS","tree","randomForest","TeachingDemos","mice"))
```
и подключаются
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(corrplot)
library(psych)
library(MASS)
library(tree)
library(randomForest)
library(TeachingDemos)
library(mice)
```

Также использовался номер варианта
```{r}
#номер варианта
nv=7
```
Дополнительная информация: 
[огромная статья по построению моделей в R](https://ranalytics.github.io/data-mining/01-Data-Mining-Models-in-R.html), [cтатья по кластерному, факторному и дискриминантному анализу в R](https://rpubs.com/iezepov/e502lec7), [курс по анализу данных в R](https://stepik.org/course/129/syllabus), [курс по основам R как языка программирования](https://stepik.org/course/497/syllabus),  [курс по статистике в R](https://stepik.org/course/524/syllabus), [перечень основых функций языка](https://aakinshin.net/ru/posts/r-functions/),  [математические операции в R](https://ru.wikibooks.org/wiki/Язык_программирования_R/Математика),  [книги по R на моём GitHub](https://github.com/PasaOpasen/LittleHelps/tree/master/книги%20по%20языкам/R).

***
## Многомерный анализ

### Задание 1
Подготовим данные:

```{r}
datacrude =data.frame(read_excel("Таблица 1.xlsx")) #считывание таблицы
data=datacrude[5:nrow(datacrude),-1]#удаление лишних строк и столбцов
data=data[-nv,]#удаление строки в соответствиии с номером варианта
colnames(data)=c("Country","Doctors","Deaths","GDP","Costs")#переименование столбцов

data[,2:5]=apply(data[,2:5],2,function(x)scale(as.numeric(x)))#тут переменные из текста преобразуются в числа и стандартизируются
data[,1]=factor(data[,1])#первая переменная из количественной преобразуется в номенативную
```
Полученная таблица:
```{r}
data_frame(data)
```

Для решения задачи создается матрица (евклидовых) расстояний
```{r}
d = dist(data[,2:5], method = "euclidean")#матрица расстояний
```
которая используется функцией кластеризации по методу ближайшего соседа с расстоянием Варда между кластерами:
```{r}
fit <- hclust(d, method = "ward.D")
```
Дендрограмма полученной кластеризации:
```{r,fig.height=7}
plot(fit, labels = data$Country,xlab = "Countries")
```

и сумма внутригрупповых расстояний по мере объединения кластеров:
```{r}
plot(fit$height, xlab ="step",ylab="dist",type="b",col="blue",lwd=1,main="Расстояния при объединении кластеров")
```

### Задание 2
Попробуем узнать, сколько кластеров будет достаточно. Для этого рассчитаем суммы внутригрупповых расстояний, когда число кластеров равно 1, 2, ... 8, и изобразим их на графике:
```{r}
it=1:8
sums=sapply(it, function(k) kmeans(data[,2:5], k)$tot.withinss)
plot(it,sums,type = "b",col="red",main = "Суммы внутригрупповых расстояний при разном числе кластеров")
```

По принципу метода каменистой осыпи делаем вывод, что исходный набор данных естественно делится на 2 или 3 кластера. Напишем функцию, которая строит модель для заданного числа кластеров, проводит анализ этой модели и строит некоторые графики:
```{r chunk1}
#функция, проводящая некоторый анализ и строящая графики для заданного числа кластеров
getimage=function(k){
  fit=kmeans(data[,2:5],k)#строится модель
cat("Внутригрупповые суммы:",fit$withinss,"\n")#внутригрупповые суммы
cat("Общая сумма:", fit$betweenss,"\n") 
cat("Матрица расстояний:\n")
print(dist(fit$centers))#матрица расстояний

#Добавляем кластер к фрейму данных
library(dplyr)
newdata=as_data_frame(data)%>%mutate(cluster=factor(fit$cluster))

#агрегирование данных по группам
means=newdata[,2:6]%>%group_by(cluster)%>%summarise(
  meanCosts=mean(Costs),sdCosts=sd(Costs),
  meanDoctors=mean(Doctors),sdDoctors=sd(Doctors),
  meanGDP=mean(GDP),sdGDP=sd(GDP),
  meanDeaths=mean(Deaths),sdDeaths=sd(Deaths)
  )
print(means)


means=means[,c(1,2,4,6,8)]#берётся сабсет только из значений для средних

lbs=c("cluster1","cluster2","cluster3","cluster4","cluster5")

library(ggplot2)
library(ggpubr)

#здесь создаётся таблица со средними по каждой переменной и каждому классу в том виде, в каком удобней рисовать
tmpdata=data.frame(x=1:4,means=as.numeric(means[1,2:5]),cluster=rep(lbs[1],4))
for(i in 2:k){
  tmpdata=rbind(tmpdata,data.frame(x=1:4,means=as.numeric(means[i,2:5]),cluster=rep(lbs[i],4)))
}
tmpdata$cluster=factor(tmpdata$cluster)

ppp=ggplot(tmpdata,aes(x=x,y=means,col=cluster))+
  geom_line()+
  geom_point(size=4)


pl1=ggplot(newdata, aes(x=Doctors, y=Deaths, col = cluster))+
  geom_point(size = 3)+
  theme_bw() 

pl2=ggplot(newdata, aes(x=GDP, y=Costs, col = cluster))+
  geom_point(size = 3)+
  theme_bw()

pl3=ggplot(newdata, aes(x=GDP, y=Deaths, col = cluster))+
  geom_point(size = 3)+
  theme_bw()
pl4=ggplot(newdata, aes(x=GDP, y=Doctors, col = cluster))+
  geom_point(size = 3)+
  theme_bw()

costs = ggplot(newdata, aes(x=cluster, y=Costs))+
  geom_boxplot()+
  theme_bw()
deaths = ggplot(newdata, aes(x=cluster, y=Deaths))+
  geom_boxplot()+
  theme_bw()
doctors = ggplot(newdata, aes(x=cluster, y=Doctors))+
  geom_boxplot()+
  theme_bw()
gdp = ggplot(newdata, aes(x=cluster, y=GDP))+
  geom_boxplot()+
  theme_bw()

p1 <- ggarrange(pl1, pl2,pl3,pl4,
                ncol = 2, nrow = 2)
p2 <- ggarrange(costs, deaths, doctors, gdp,
                ncol = 2, nrow = 2)
ggarrange(ppp,p1, p2, ncol = 1, nrow = 3,heights=c(1.3,2,3))
}
```

Используем эту функцию для двух кластеров:
```{r,cache=TRUE,fig.height=10}
getimage(2)
```

Видно, что исходные наблюдения хорошо разделяются на две группы. Если использовать три кластера, такое разделение будет уже сомнительным:
```{r,cache=TRUE,fig.height=10}
getimage(3)
```

Ещё один способ оценивать качество разбиения на группы --- **лица Чернова**. Идея состоит в том, чтобы для каждой группы найти какую-то характеристику каждой переменной (например, среднее), а затем на основе вектора таких характеристик постоить лица каждой группы: чем лица более похожи, тем группы более близки. Сделаем так для двух кластеров:
```{r}
#функция делает анализ dataset по методу k-means с k кластерами, затем добавляет результаты к датасету
getbykmeans=function(dataset,k){
  fit=kmeans(dataset,k)#строится модель
  #Добавляем кластер к фрейму данных
  library(dplyr)
  newdata=as_data_frame(dataset)%>%mutate(cluster=factor(fit$cluster))
}
#функция считает средние и рисует лица
getfaces=function(k){
  #создаем матрицу средних
means=getbykmeans(data[,2:5],k)%>%group_by(cluster)%>%
  summarise_all(funs(mean))

library(TeachingDemos)
faces(means[,2:5])#рисуем лица
}
getfaces(2)
```

Видно, что лица сильно различаются. Для трёх кластеров
```{r}
getfaces(3)
```

два лица уже достаточно похожи. Для четырёх кластеров имеется две пары очень похожих лиц:
```{r}
getfaces(4)
```


Отсюда вывод: достаточно было использовать только два кластера.


### Задание 3

Загрузим данные:
```{r}
datacrude =data.frame(read_excel("Приложение 1.xlsx")) 
data=datacrude[,-c(1)]
data=data[,-c(1,2,16,17)]
data_frame(data)
```

Визуализация корреляционной матрицы заданного набора переменных:
```{r}
library(corrplot)
corrplot(cor(data))
```

В наборе данных в целом нет значительных корреляций, поэтому сжать его до трёх-четырёх главных компонент без сильной потери информации не получится.

Результаты аналаза методом главных компонент без вращения:
```{r}
library(psych)
principal(data[,-1],nfactors = 8,rotate = "none") #Создание модели
```

Здесь сперва выведена матрица корреляций, затем **SS loadings** --- это собственные значения каждой компоненты, **Proportion Var** --- доля дисперсий, объясняемых каждой компонентой, **Cumulative Var** --- кумулятивная доля (первая компонента объясняет 20%, первые две вместе --- 34%, первые три --- 46% и т. д.), дальше то же самое, только в пропорции. По принципу Кайзера следует выделить только 5 компонент, хотя, судя по объяснённым дисперсиям, и восьми может быть недостаточно.

Построим диаграмму каменистой осыпи:
```{r,message=FALSE}
fa.parallel(data[,-1],fa="pc",show.legend = T,main="Диаграмма каменистой осыпи с параллельным анализом")

```

Здесь, как и ожидалось, не наблюдается резкого убывания собственных значений, поэтому нельзя сказать, сколько конкретно главных компонент следовало бы выделить. Возмём 6 факторов и проведём анализ с вращением осей:
```{r}
#варимакс с нормализацией
(vm=principal(apply(data[,-1],2,scale),nfactors = 6,rotate = "varimax"))
```

По результатам анализа видно, что первая компонента сильно коррелирует с переменной Х13 (среднегодовой фонд заработной платы), пятая --- с Х6 (удельный вес покупных изделий) и Х15 (оборачиваемость нормированных оборотных средств), третья --- с Х10 (фондоотдача) и Х9 (удельный вес потерь от брака), вторая --- с Х11 (среднегодовая численность ППП), четвёртая --- с Х5 (удельный вес рабочих в составе ППП), шестая --- с Х7 (коэффициент сменности оборудования).

Также можно увидеть матрицу весовых коэффициентов:
```{r}
round(unclass(vm$weights),2)
```
а корреляционная матрица для главных компонент показывает их ортогональность:
```{r}
cor(vm$scores)
```

### Задание 4
Загрузим данные:
```{r}
data =data.frame(read_excel("Приложение 2.xlsx")) 
data$CLASS=factor(data$CLASS)
head(data,9)
```
Визуализировать эти данные будет не лучшей идей, так переменных и кластеров довольно много:
```{r,fig.height=10,fig.width=10}
pairs(data[,1:7],col=data$CLASS,pch=16)
```

Лица Чернова показывают, что в двух случаях между парами групп сильной разницы нет:
```{r}
newdata=as_data_frame(data)%>%group_by(CLASS)%>%
    summarise_all(funs(mean))
  faces(newdata[,2:8])#рисуем лица
```


Проведём обучение через линейный дискриминантный анализ:
```{r, include=FALSE}
library(MASS)
ldadat <- lda(CLASS~.,data,method="t")
```
```{r, eval=FALSE}
library(MASS)
ldadat <- lda(CLASS~.,data,method="t")
```
Характеристики модели:
```{r}
ldadat$means#групповые средние
(mat=ldadat$scaling)#матрица дискриминантных функций
```
Напишем функцию для удобной оценки ошибки обучения
```{r}
#функция для оценки ошибки 
misclass <- function(pred, obs) {
   tbl <- table(pred, obs)
   sum <- colSums(tbl)
   dia <- diag(tbl)
   msc <- (sum - dia)/sum * 100
   m.m <- mean(msc)
   cat("Classification table:", "\n")
   print(tbl)
   cat("Misclassification errors:", "\n")
   print(round(msc, 1))
  }
```
И применим её для значений, предсказанных моделью на обучающей выборке:
```{r}
misclass(predict(ldadat, data[,1:7])$class, data[,8])
```
Видно, что для некоторых переменных имеется ошибка более чем в 30%, что означает негодность выбранной модели обучения. Модель "Breiman's random forest" справляется с этой задачей лучше:
```{r,fig.height=7,fig.width=7}
library(randomForest)
rf <- randomForest(data[,8] ~ ., data=data[,1:7])
rfp <- predict(rf, data[,1:7])
MDSplot(randomForest(data[,-8]), data[,8])
misclass(rfp, data[,8])
```
Ошибка нулевая, все данные отнесены правильно. Правило, по которому наблюдение относится в ту или иную группу примерно сделующее:
```{r}
library(tree)
datatree <- tree(data[,8] ~ ., data[,-8])
plot(datatree)
text(datatree) 
```

### Задание 5

Считаем тестовые данные и проведём их классификацию по уже построенной модели:
```{r}
data2 =data.frame(read_excel("Приложение 3.xlsx")) 
data2= apply(data2,2,as.numeric)
data2=data2[31:80,]
cluster=predict(rf, data2)
data2=data.frame(cbind(data2,cluster))
data2$cluster=factor(data2$cluster)

head(data2,10)
```
Построим диаграммы ядерной плотности для распределения каждой переменной X1-X7 по отнесенным группам, чтобы убедиться в правильности классификации:
```{r,fig.height=9, cache=TRUE}
library(ggplot2)
library(ggpubr)

ggarrange(
  ggplot(data2,aes(x=X1,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X2,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X3,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X4,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X5,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X6,fill=cluster))+
    geom_density(alpha=0.6),
  ggplot(data2,aes(x=X7,fill=cluster))+
    geom_density(alpha=0.6),
          ncol = 2, nrow = 4)
```

Другой вариант --- ящики с усами:
```{r,fig.height=9, cache=TRUE}
ggarrange(
  ggplot(data2,aes(x=cluster,y=X1))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X2))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X3))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X4))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X5))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X6))+
    geom_boxplot(),
  ggplot(data2,aes(x=cluster,y=X7))+
    geom_boxplot(),
  ncol = 2, nrow = 4)
```

Также лица Чернова:
```{r, cache=TRUE}
newdata=as_data_frame(data2)%>%group_by(cluster)%>%
    summarise_all(funs(mean))
  faces(newdata[,2:8])#рисуем лица
```

В целом качество такое же, как в обучающей выборке.















***

## Временные ряды
Используются две переменные:
```{r}
p1 = nchar("Дмитрий")#число букв в имени
p2 = nchar("Пасько")#число букв в фамилии
```

### Задание 1

### Задание 3
Создадим и визуализируем временной ряд:
```{r}
library(ggplot2)

yt=c(1133+ p1,	1222,	1354+ p1,	1389,	1342+ p2,	1377,	1491,	1684+ p2)
data=data.frame(time=1:length(yt),values=yt)
plot(data,type="b")
```

В целом, здесь наблюдается линейная составляющая. Построим линейную модель и регрессионную прямую:
```{r}
fit=lm(values~time,data)#создание модели
ggplot(data,aes(x=time,y=values))+
  geom_point()+
  geom_smooth(method=lm)
```

Посмотрим информацию о модели:
```{r}
summary(fit)#информация о модели
```
Самое важное здесь --- модель описывает более 80% дисперсий (**Adjusted R-squared**), каждый её коэффициент (**Pr(>|t|)**) и она сама (**p-value**) статистически значимы. **Estimate** --- это коэффициенты модели, значит сама регресионная прямая имеет вид $$y=1098.57 + 61.93 t$$

Теперь выполним прогноз для среднего и индивидульного значений при $t=9$: 
```{r}
#прогноз среднего
predict(fit,data.frame(time = c(9)), se.fit=TRUE, interval="confidence", level=0.95)$fit
#прогноз индивидуального
predict(fit,data.frame(time = c(9)), se.fit=TRUE, interval="prediction", level=0.95)$fit
```


```{r}
```
Здесь первое число --- само предсказанное значение, последующие --- границы доверительного интервала.

### Задание 4
Считаем набор данных и проведём некоторую обработку:
```{r, message=FALSE}
library(readxl)
data=data.frame(read_xlsx("РожьВсеГода.xlsx"))
data[,-1]=apply(data[,-1], 2, as.numeric)#перевести в числа все строки
y=t(data[,-1])#транспонирование для удобства

#получить массив лет
ns=rownames(y)
x=sapply(ns, function(s) as.numeric(substr(s,2,nchar(s))))

library(mice)#обработать пустые значения
imp=mice(y,seed=11)
y=complete(imp,action = 1)

df=data.frame(x=x,y=y[,2])#объединить данные в фрейм

print(df[sort(sample(1:nrow(df),13)),2,drop=FALSE]) #вывести 13 случайных строк
```
При этом пропущенные значения были обработаны по алгоритму [MICE](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.5745&rep=rep1&type=pdf), для дальнейшего исследования был взят второй район. Визуализировав данные по нему
```{r}
library(ggplot2)
ggplot(df,aes(x=x,y=y))+
  geom_line(col="green")+
  geom_point(size=2)+
  geom_smooth(method = lm)+
  geom_smooth(se=F,col="red")
```

приходим к выводу, что в целом поведение цен можно описать линейной моделью, однако для обычной линейной модели здесь явно не будет выполняться требование гомоскедаксичности. Попробуем разные преобразования данных:
```{r}
summary(lm(y~x,df))
summary(lm(sqrt(y)~x,df))
summary(lm(log(y)~x,df))
summary(lm(log(y)~log(x),df))
```
Видно, что модель с логарифмами описывает почти 85% дисперсий, поэтому в дальнейшем будем использовать её. Построи тот же график:
```{r}
library(ggplot2)
ggplot(df,aes(x=log(x),y=log(y)))+
  geom_line(col="green")+
  geom_point(size=2)+
  geom_smooth(method = lm)+
  geom_smooth(se=F,col="red")
```

Для остатков полученной модели проведём сглаживание методом k-средних:
```{r}
mt=lm(log(y)~log(x),df)
res=mt$residuals
#скользящее среднее
library(caTools)
k=c(3,5,9)
plot(x,res,type="l",col="grey")
for(i in 1:length(k)){
  lines(x,runmean(res,k[i]),col=i,lwd=2)
}

legend("topleft",c(paste("k =", k)),col=1:length(k),bty="n",lwd=2)
```

И последнее: визуализируем корреляции между всеми исходными районами за последние 60 лет наблюдений по отрезкам в 20 лет:
```{r}
library(corrplot)
nn=20

for(i in seq(length(x)-60,length(x)-nn,nn)){
  tmp=i:(i+nn-1)
  cat("Times:",x[tmp],"\n")
  data=y[tmp,]#транспонирование, чтобы строки стали переменными
  cormatrix=cor(data)
  lower=abs(cormatrix[lower.tri(cormatrix)])
  cat("Статистика по треугольнику корреляционной матрицы \n")
  print(summary(lower[lower!=0])) 
  corrplot(cormatrix)
}
```
Наглядно видно, что ближе к концу наблюдений корреляция между переменными возрастает.



